{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/hp/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import operator\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from random import shuffle\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "#PLOT CONFUSION MATRIX\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import pickle\n",
    "#matrix inverse\n",
    "from numpy.linalg import inv\n",
    "import jj_basic_fn as JJ\n",
    "from hyperparams import Hyperparams as hp\n",
    "from patient import patient\n",
    "import prep\n",
    "import plot_funcs\n",
    "import modules\n",
    "#default size of the graph\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "n_classifier = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/hp/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:337: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n"
     ]
    }
   ],
   "source": [
    "p231, p222_1, p222_2, p222_3, p229 = modules.build_patients()\n",
    "pat_list = [p231, p222_1, p222_2, p222_3]\n",
    "for pat in pat_list:\n",
    "    JJ.save_object(pat, '../patients/' + pat.id +'.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_rs = hp.col_rs\n",
    "col_es = hp.col_es\n",
    "col_le = hp.col_le\n",
    "\n",
    "p231, p222_1, p222_2, p222_3 = pickle.load(open(hp.prepath_pat + \"231.p\", \"rb\" )),pickle.load(open(hp.prepath_pat + \"222_1.p\", \"rb\" )), pickle.load(open(hp.prepath_pat + \"222_2.p\", \"rb\" )), pickle.load(open(hp.prepath_pat + \"222_3.p\", \"rb\" ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>region_start_time</th>\n",
       "      <th>sleep</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "      <th>theta1</th>\n",
       "      <th>theta2</th>\n",
       "      <th>theta3</th>\n",
       "      <th>...</th>\n",
       "      <th>all1</th>\n",
       "      <th>all2</th>\n",
       "      <th>all3</th>\n",
       "      <th>all4</th>\n",
       "      <th>i12</th>\n",
       "      <th>i34</th>\n",
       "      <th>epoch</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>if_stimulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1.309980e+17</td>\n",
       "      <td>2016-02-12 03:59:55.968000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.436218</td>\n",
       "      <td>364.151777</td>\n",
       "      <td>63.215202</td>\n",
       "      <td>202.719760</td>\n",
       "      <td>101.081960</td>\n",
       "      <td>344.757574</td>\n",
       "      <td>133.174825</td>\n",
       "      <td>...</td>\n",
       "      <td>393.474657</td>\n",
       "      <td>1179.278104</td>\n",
       "      <td>502.973709</td>\n",
       "      <td>872.480713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>1.309980e+17</td>\n",
       "      <td>2016-02-12 15:59:53.030400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.952992</td>\n",
       "      <td>206.077023</td>\n",
       "      <td>34.415236</td>\n",
       "      <td>91.723497</td>\n",
       "      <td>102.638616</td>\n",
       "      <td>395.431395</td>\n",
       "      <td>87.026204</td>\n",
       "      <td>...</td>\n",
       "      <td>288.098271</td>\n",
       "      <td>1034.826536</td>\n",
       "      <td>407.075884</td>\n",
       "      <td>805.376122</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1.309989e+17</td>\n",
       "      <td>2016-02-12 20:59:51.964800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.293362</td>\n",
       "      <td>244.848856</td>\n",
       "      <td>34.561427</td>\n",
       "      <td>66.295568</td>\n",
       "      <td>63.117262</td>\n",
       "      <td>448.411932</td>\n",
       "      <td>85.858412</td>\n",
       "      <td>...</td>\n",
       "      <td>214.307536</td>\n",
       "      <td>995.629253</td>\n",
       "      <td>477.849916</td>\n",
       "      <td>709.906735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>1.309989e+17</td>\n",
       "      <td>2016-02-13 03:59:41.020800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.736205</td>\n",
       "      <td>121.930635</td>\n",
       "      <td>23.548927</td>\n",
       "      <td>73.607882</td>\n",
       "      <td>58.570117</td>\n",
       "      <td>291.327793</td>\n",
       "      <td>73.162509</td>\n",
       "      <td>...</td>\n",
       "      <td>199.280474</td>\n",
       "      <td>765.134289</td>\n",
       "      <td>423.093376</td>\n",
       "      <td>693.146568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1.309989e+17</td>\n",
       "      <td>2016-02-13 15:59:39.033600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.837088</td>\n",
       "      <td>159.177819</td>\n",
       "      <td>29.398845</td>\n",
       "      <td>97.541387</td>\n",
       "      <td>87.276202</td>\n",
       "      <td>475.231517</td>\n",
       "      <td>75.751883</td>\n",
       "      <td>...</td>\n",
       "      <td>256.451487</td>\n",
       "      <td>1035.538565</td>\n",
       "      <td>379.213366</td>\n",
       "      <td>822.637315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>1.309997e+17</td>\n",
       "      <td>2016-02-13 20:59:37.968000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.313928</td>\n",
       "      <td>132.279432</td>\n",
       "      <td>17.281833</td>\n",
       "      <td>74.976220</td>\n",
       "      <td>68.591295</td>\n",
       "      <td>388.279137</td>\n",
       "      <td>68.100650</td>\n",
       "      <td>...</td>\n",
       "      <td>225.398151</td>\n",
       "      <td>858.877318</td>\n",
       "      <td>416.540262</td>\n",
       "      <td>790.444117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1.309997e+17</td>\n",
       "      <td>2016-02-14 03:59:36.960000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.166778</td>\n",
       "      <td>273.677298</td>\n",
       "      <td>33.567358</td>\n",
       "      <td>81.248635</td>\n",
       "      <td>67.960011</td>\n",
       "      <td>407.512272</td>\n",
       "      <td>63.612451</td>\n",
       "      <td>...</td>\n",
       "      <td>246.179542</td>\n",
       "      <td>1191.769707</td>\n",
       "      <td>405.645495</td>\n",
       "      <td>690.393928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>1.309997e+17</td>\n",
       "      <td>2016-02-14 15:59:34.022400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.434457</td>\n",
       "      <td>146.502404</td>\n",
       "      <td>26.545324</td>\n",
       "      <td>113.718263</td>\n",
       "      <td>92.329255</td>\n",
       "      <td>422.707738</td>\n",
       "      <td>83.928926</td>\n",
       "      <td>...</td>\n",
       "      <td>263.857999</td>\n",
       "      <td>988.330991</td>\n",
       "      <td>427.060643</td>\n",
       "      <td>896.721623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>1.310006e+17</td>\n",
       "      <td>2016-02-14 20:59:33.993600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.334224</td>\n",
       "      <td>375.483229</td>\n",
       "      <td>49.405315</td>\n",
       "      <td>121.113454</td>\n",
       "      <td>92.537941</td>\n",
       "      <td>457.397495</td>\n",
       "      <td>93.631180</td>\n",
       "      <td>...</td>\n",
       "      <td>364.616993</td>\n",
       "      <td>1360.944809</td>\n",
       "      <td>473.557109</td>\n",
       "      <td>802.123650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>1.310006e+17</td>\n",
       "      <td>2016-02-15 03:59:22.963200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.442024</td>\n",
       "      <td>143.936747</td>\n",
       "      <td>40.430945</td>\n",
       "      <td>101.728189</td>\n",
       "      <td>40.123491</td>\n",
       "      <td>232.779533</td>\n",
       "      <td>87.369548</td>\n",
       "      <td>...</td>\n",
       "      <td>193.653876</td>\n",
       "      <td>776.878225</td>\n",
       "      <td>372.658872</td>\n",
       "      <td>673.987872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>1.312097e+17</td>\n",
       "      <td>2016-10-13 20:52:13.958400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.849526</td>\n",
       "      <td>130.582828</td>\n",
       "      <td>21.782559</td>\n",
       "      <td>77.280066</td>\n",
       "      <td>85.145398</td>\n",
       "      <td>392.384598</td>\n",
       "      <td>109.818866</td>\n",
       "      <td>...</td>\n",
       "      <td>247.567088</td>\n",
       "      <td>922.099081</td>\n",
       "      <td>606.518352</td>\n",
       "      <td>810.325312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>1.312097e+17</td>\n",
       "      <td>2016-10-14 03:52:01.977600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.480218</td>\n",
       "      <td>166.409104</td>\n",
       "      <td>38.253607</td>\n",
       "      <td>86.272224</td>\n",
       "      <td>73.847559</td>\n",
       "      <td>328.566468</td>\n",
       "      <td>76.675459</td>\n",
       "      <td>...</td>\n",
       "      <td>242.849650</td>\n",
       "      <td>1057.874291</td>\n",
       "      <td>429.902087</td>\n",
       "      <td>733.484519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>1.312097e+17</td>\n",
       "      <td>2016-10-14 15:51:59.990400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.472554</td>\n",
       "      <td>121.919630</td>\n",
       "      <td>32.526010</td>\n",
       "      <td>87.067608</td>\n",
       "      <td>100.492996</td>\n",
       "      <td>349.743992</td>\n",
       "      <td>76.188373</td>\n",
       "      <td>...</td>\n",
       "      <td>290.464335</td>\n",
       "      <td>963.307718</td>\n",
       "      <td>422.321791</td>\n",
       "      <td>761.239581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>1.312106e+17</td>\n",
       "      <td>2016-10-14 20:51:57.974400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.956015</td>\n",
       "      <td>122.934611</td>\n",
       "      <td>22.868977</td>\n",
       "      <td>65.386303</td>\n",
       "      <td>79.506908</td>\n",
       "      <td>321.541046</td>\n",
       "      <td>104.788589</td>\n",
       "      <td>...</td>\n",
       "      <td>240.579613</td>\n",
       "      <td>906.777277</td>\n",
       "      <td>475.430731</td>\n",
       "      <td>768.482556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>1.312106e+17</td>\n",
       "      <td>2016-10-15 03:51:56.966400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.483495</td>\n",
       "      <td>241.823225</td>\n",
       "      <td>84.759599</td>\n",
       "      <td>171.891446</td>\n",
       "      <td>69.457314</td>\n",
       "      <td>367.642559</td>\n",
       "      <td>156.888956</td>\n",
       "      <td>...</td>\n",
       "      <td>282.585202</td>\n",
       "      <td>1189.297221</td>\n",
       "      <td>672.973876</td>\n",
       "      <td>985.201459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>1.312106e+17</td>\n",
       "      <td>2016-10-15 15:51:54.979200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.213590</td>\n",
       "      <td>110.498612</td>\n",
       "      <td>29.145385</td>\n",
       "      <td>77.395535</td>\n",
       "      <td>97.463437</td>\n",
       "      <td>359.090868</td>\n",
       "      <td>87.032101</td>\n",
       "      <td>...</td>\n",
       "      <td>259.456104</td>\n",
       "      <td>979.014053</td>\n",
       "      <td>411.963965</td>\n",
       "      <td>859.602739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>1.312114e+17</td>\n",
       "      <td>2016-10-15 20:51:54.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.787801</td>\n",
       "      <td>104.188756</td>\n",
       "      <td>21.657637</td>\n",
       "      <td>75.277413</td>\n",
       "      <td>75.386855</td>\n",
       "      <td>304.362008</td>\n",
       "      <td>89.117432</td>\n",
       "      <td>...</td>\n",
       "      <td>250.096486</td>\n",
       "      <td>822.743905</td>\n",
       "      <td>496.960676</td>\n",
       "      <td>867.736286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>1.312114e+17</td>\n",
       "      <td>2016-10-16 03:51:42.969600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116.646185</td>\n",
       "      <td>585.714496</td>\n",
       "      <td>118.107709</td>\n",
       "      <td>291.785970</td>\n",
       "      <td>127.110164</td>\n",
       "      <td>494.224383</td>\n",
       "      <td>160.089297</td>\n",
       "      <td>...</td>\n",
       "      <td>392.468064</td>\n",
       "      <td>1609.738531</td>\n",
       "      <td>585.400155</td>\n",
       "      <td>1150.547519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>1.312114e+17</td>\n",
       "      <td>2016-10-16 15:51:40.032000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.538403</td>\n",
       "      <td>373.470750</td>\n",
       "      <td>43.846616</td>\n",
       "      <td>100.506508</td>\n",
       "      <td>148.405154</td>\n",
       "      <td>378.227376</td>\n",
       "      <td>92.554714</td>\n",
       "      <td>...</td>\n",
       "      <td>470.190253</td>\n",
       "      <td>1369.334836</td>\n",
       "      <td>625.358306</td>\n",
       "      <td>1041.620044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>1.312122e+17</td>\n",
       "      <td>2016-10-16 20:51:38.966400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.168131</td>\n",
       "      <td>104.536728</td>\n",
       "      <td>17.215758</td>\n",
       "      <td>73.935835</td>\n",
       "      <td>61.085664</td>\n",
       "      <td>290.991518</td>\n",
       "      <td>104.835974</td>\n",
       "      <td>...</td>\n",
       "      <td>206.057316</td>\n",
       "      <td>790.201809</td>\n",
       "      <td>539.955091</td>\n",
       "      <td>828.139389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename          region_start_time  sleep      delta1      delta2  \\\n",
       "521   1.309980e+17 2016-02-12 03:59:55.968000  1.0    65.436218   364.151777   \n",
       "522   1.309980e+17 2016-02-12 15:59:53.030400  0.0    66.952992   206.077023   \n",
       "523   1.309989e+17 2016-02-12 20:59:51.964800  0.0    39.293362   244.848856   \n",
       "524   1.309989e+17 2016-02-13 03:59:41.020800  1.0    37.736205   121.930635   \n",
       "525   1.309989e+17 2016-02-13 15:59:39.033600  0.0    48.837088   159.177819   \n",
       "526   1.309997e+17 2016-02-13 20:59:37.968000  0.0    41.313928   132.279432   \n",
       "527   1.309997e+17 2016-02-14 03:59:36.960000  1.0    61.166778   273.677298   \n",
       "528   1.309997e+17 2016-02-14 15:59:34.022400  0.0    56.434457   146.502404   \n",
       "529   1.310006e+17 2016-02-14 20:59:33.993600  0.0    78.334224   375.483229   \n",
       "530   1.310006e+17 2016-02-15 03:59:22.963200  1.0    50.442024   143.936747   \n",
       "...            ...                        ...  ...          ...          ...   \n",
       "1239  1.312097e+17 2016-10-13 20:52:13.958400  0.0    41.849526   130.582828   \n",
       "1240  1.312097e+17 2016-10-14 03:52:01.977600  1.0    45.480218   166.409104   \n",
       "1241  1.312097e+17 2016-10-14 15:51:59.990400  0.0    55.472554   121.919630   \n",
       "1242  1.312106e+17 2016-10-14 20:51:57.974400  0.0    40.956015   122.934611   \n",
       "1243  1.312106e+17 2016-10-15 03:51:56.966400  1.0    63.483495   241.823225   \n",
       "1244  1.312106e+17 2016-10-15 15:51:54.979200  0.0    44.213590   110.498612   \n",
       "1245  1.312114e+17 2016-10-15 20:51:54.000000  0.0    37.787801   104.188756   \n",
       "1246  1.312114e+17 2016-10-16 03:51:42.969600  1.0    116.646185  585.714496   \n",
       "1247  1.312114e+17 2016-10-16 15:51:40.032000  0.0    73.538403   373.470750   \n",
       "1248  1.312122e+17 2016-10-16 20:51:38.966400  0.0    38.168131   104.536728   \n",
       "\n",
       "          delta3      delta4      theta1      theta2      theta3  \\\n",
       "521   63.215202   202.719760  101.081960  344.757574  133.174825   \n",
       "522   34.415236   91.723497   102.638616  395.431395  87.026204    \n",
       "523   34.561427   66.295568   63.117262   448.411932  85.858412    \n",
       "524   23.548927   73.607882   58.570117   291.327793  73.162509    \n",
       "525   29.398845   97.541387   87.276202   475.231517  75.751883    \n",
       "526   17.281833   74.976220   68.591295   388.279137  68.100650    \n",
       "527   33.567358   81.248635   67.960011   407.512272  63.612451    \n",
       "528   26.545324   113.718263  92.329255   422.707738  83.928926    \n",
       "529   49.405315   121.113454  92.537941   457.397495  93.631180    \n",
       "530   40.430945   101.728189  40.123491   232.779533  87.369548    \n",
       "...         ...          ...        ...          ...        ...    \n",
       "1239  21.782559   77.280066   85.145398   392.384598  109.818866   \n",
       "1240  38.253607   86.272224   73.847559   328.566468  76.675459    \n",
       "1241  32.526010   87.067608   100.492996  349.743992  76.188373    \n",
       "1242  22.868977   65.386303   79.506908   321.541046  104.788589   \n",
       "1243  84.759599   171.891446  69.457314   367.642559  156.888956   \n",
       "1244  29.145385   77.395535   97.463437   359.090868  87.032101    \n",
       "1245  21.657637   75.277413   75.386855   304.362008  89.117432    \n",
       "1246  118.107709  291.785970  127.110164  494.224383  160.089297   \n",
       "1247  43.846616   100.506508  148.405154  378.227376  92.554714    \n",
       "1248  17.215758   73.935835   61.085664   290.991518  104.835974   \n",
       "\n",
       "          ...              all1         all2        all3         all4  i12  \\\n",
       "521       ...        393.474657  1179.278104  502.973709  872.480713   0.0   \n",
       "522       ...        288.098271  1034.826536  407.075884  805.376122   1.0   \n",
       "523       ...        214.307536  995.629253   477.849916  709.906735   0.0   \n",
       "524       ...        199.280474  765.134289   423.093376  693.146568   0.0   \n",
       "525       ...        256.451487  1035.538565  379.213366  822.637315   0.0   \n",
       "526       ...        225.398151  858.877318   416.540262  790.444117   0.0   \n",
       "527       ...        246.179542  1191.769707  405.645495  690.393928   0.0   \n",
       "528       ...        263.857999  988.330991   427.060643  896.721623   1.0   \n",
       "529       ...        364.616993  1360.944809  473.557109  802.123650   0.0   \n",
       "530       ...        193.653876  776.878225   372.658872  673.987872   0.0   \n",
       "...       ...               ...         ...          ...         ...   ...   \n",
       "1239      ...        247.567088  922.099081   606.518352  810.325312   0.0   \n",
       "1240      ...        242.849650  1057.874291  429.902087  733.484519   0.0   \n",
       "1241      ...        290.464335  963.307718   422.321791  761.239581   0.0   \n",
       "1242      ...        240.579613  906.777277   475.430731  768.482556   0.0   \n",
       "1243      ...        282.585202  1189.297221  672.973876  985.201459   0.0   \n",
       "1244      ...        259.456104  979.014053   411.963965  859.602739   0.0   \n",
       "1245      ...        250.096486  822.743905   496.960676  867.736286   0.0   \n",
       "1246      ...        392.468064  1609.738531  585.400155  1150.547519  0.0   \n",
       "1247      ...        470.190253  1369.334836  625.358306  1041.620044  0.0   \n",
       "1248      ...        206.057316  790.201809   539.955091  828.139389   0.0   \n",
       "\n",
       "      i34  epoch  label     id  if_stimulated  \n",
       "521   0.0  0      True   222_1  False          \n",
       "522   0.0  0      True   222_1  False          \n",
       "523   0.0  0      True   222_1  False          \n",
       "524   0.0  0      True   222_1  False          \n",
       "525   0.0  0      True   222_1  False          \n",
       "526   1.0  0      True   222_1  False          \n",
       "527   0.0  0      True   222_1  False          \n",
       "528   1.0  0      True   222_1  False          \n",
       "529   0.0  0      True   222_1  False          \n",
       "530   0.0  0      True   222_1  False          \n",
       "...   ... ..       ...     ...    ...          \n",
       "1239  1.0  7      False  222_1  False          \n",
       "1240  0.0  7      False  222_1  False          \n",
       "1241  0.0  7      False  222_1  False          \n",
       "1242  1.0  7      False  222_1  False          \n",
       "1243  0.0  7      False  222_1  False          \n",
       "1244  0.0  7      False  222_1  False          \n",
       "1245  0.0  7      False  222_1  False          \n",
       "1246  0.0  7      False  222_1  False          \n",
       "1247  0.0  7      False  222_1  False          \n",
       "1248  0.0  7      False  222_1  False          \n",
       "\n",
       "[728 rows x 37 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p222_1.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_tuning(pat, X_train, X_test, y_train, y_test, classifier, C_range_num = 30, if_save = 0,\n",
    "                     nfold = 10, if_show = 1):\n",
    "    #defs is a dictionary to initiate classifier with the parameters that don't need to be tuned\n",
    "    defs = {}\n",
    "    defs['classifier'] = classifier\n",
    "    \n",
    "    num_instances, num_features = X_train.shape[0], X_train.shape[1]\n",
    "    n_fold = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    CV = skf.split(np.zeros(len(y_train)), y_train)\n",
    "    \n",
    "\n",
    "    if classifier==1:\n",
    "        clf_name = 'Logistic Regression'\n",
    "        defs['class_weight'] = 'balanced'\n",
    "        defs['max_iter'] = 200\n",
    "        C_range = 10 ** np.random.uniform(-2, 1, size = C_range_num)\n",
    "        tuned_params = dict(penalty=['l1','l2'], C=C_range)\n",
    "    elif classifier == 2: \n",
    "        clf_name = 'SVM'\n",
    "        defs['class_weight'] = 'balanced'\n",
    "        kernel_list = ['rbf']    \n",
    "        gamma_list = [2**i*1/num_features for i in range(1)]\n",
    "        #degree_list = [2,3,4,5]\n",
    "        C_range = 10 ** np.random.uniform(-3, 1, size = C_range_num)\n",
    "        tuned_params = dict(kernel=kernel_list,gamma = gamma_list, C=C_range)\n",
    "\n",
    "    elif classifier==3:\n",
    "        clf_name = 'Gaussian Naive Bayes classifier'\n",
    "        prr = np.ones(2) * (1.0/2)\n",
    "        defs['priors'] = prr\n",
    "        clf_try = JJ.clf_list(defs)\n",
    "        clf_try.fit(X_train, y_train)\n",
    "        pat.estimator[classifier] = clf_try\n",
    "        return\n",
    "\n",
    "    elif classifier==4:\n",
    "        clf_name = 'Linear Discriminant Analysis'\n",
    "        prr = np.ones(2) * (1.0/2)\n",
    "        defs['solver'] = 'eigen'  # 'svd', 'lsqr', 'eigen'\n",
    "        defs['shrinkage'] = 'auto'\n",
    "        defs['priors'] = prr\n",
    "        clf_try = JJ.clf_list(defs)\n",
    "        clf_try.fit(X_train, y_train)\n",
    "        pat.estimator[classifier] = clf_try\n",
    "        return\n",
    "    elif classifier == 5:\n",
    "        clf_name = 'decision tree'\n",
    "        mss_list = [5,10,20,40,60]\n",
    "        criterion_list = ['entropy']\n",
    "        max_depth_list = [3,4,5,8,12,18]\n",
    "        clf_name = 'decision tree'\n",
    "        tuned_params = dict(criterion=criterion_list, min_samples_split=mss_list, max_depth = max_depth_list)\n",
    "    elif classifier == 6:\n",
    "        clf_name = 'random forest'\n",
    "        defs['n_estimators'] = 600\n",
    "        mss_list = [20,25,30,40]\n",
    "        criterion_list = ['entropy']\n",
    "        max_depth_list = [12,13,14,15,16]\n",
    "        max_features_list = ['auto']\n",
    "        tuned_params = dict(criterion=criterion_list, min_samples_split=mss_list, max_depth = max_depth_list, max_features = max_features_list)\n",
    "    elif classifier == 7:\n",
    "        clf_name = 'gradient boosting'\n",
    "        defs['n_estimators'] = 2000\n",
    "        max_depth_list, subsample_list, learning_rate_list, min_samples_leaf_list = [1,2,3], [0.1,0.15,0.2, 0.3, 0.4], [0.02, 0.01,0.005], [10,20,30] \n",
    "        #params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,\n",
    "        #  'learning_rate': 0.01, 'min_samples_leaf': 10, 'random_state': 3}\n",
    "        tuned_params = dict(max_depth=max_depth_list, subsample = subsample_list,learning_rate = learning_rate_list, min_samples_leaf= min_samples_leaf_list)\n",
    "    \n",
    "        \n",
    "    clf_try = JJ.clf_list(defs)\n",
    "    \n",
    "    clf_grid = GridSearchCV(clf_try,\n",
    "                            param_grid=tuned_params,\n",
    "                            cv=CV,\n",
    "                            scoring = 'roc_auc',\n",
    "                            verbose=1,\n",
    "                           return_train_score = True)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    print('Best score for validations set: {}'.format(clf_grid.best_score_))\n",
    "    print('Best parameters: {}'.format(clf_grid.best_params_))\n",
    "\n",
    "    clf_best = clf_grid.best_estimator_\n",
    "\n",
    "    y_pred = clf_best.predict(X_test)\n",
    "    df = pd.DataFrame(clf_grid.cv_results_)\n",
    "    if if_show:\n",
    "        JJ.show_result(y_pred, y_test, df, clf_name, if_save = if_save)\n",
    "    \n",
    "    if if_save:\n",
    "        pat.result[classifier] = df\n",
    "        pat.estimator[classifier] = clf_best\n",
    "        pat.score[classifier] = clf_grid.best_score_\n",
    "        pat.params[classifier] = clf_grid.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 135 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1350 out of 1350 | elapsed: 30.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.850188137270412\n",
      "Best parameters: {'max_depth': 3, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.4}\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.4}</td>\n",
       "      <td>0.850188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>{'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 20, 'subsample': 0.3}</td>\n",
       "      <td>0.849860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>{'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.3}</td>\n",
       "      <td>0.849457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>{'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.4}</td>\n",
       "      <td>0.848894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>{'max_depth': 1, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.4}</td>\n",
       "      <td>0.848731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>{'max_depth': 1, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.3}</td>\n",
       "      <td>0.848631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.01, 'min_samples_leaf': 20, 'subsample': 0.2}</td>\n",
       "      <td>0.847872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>{'max_depth': 2, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.4}</td>\n",
       "      <td>0.847684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.3}</td>\n",
       "      <td>0.847291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.3}</td>\n",
       "      <td>0.846632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.02, 'min_samples_leaf': 10, 'subsample': 0.15}</td>\n",
       "      <td>0.826461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.820724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.820555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>{'max_depth': 1, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.820404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>{'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.819148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>{'max_depth': 1, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.818847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.02, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.817413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>{'max_depth': 2, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.817397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'max_depth': 1, 'learning_rate': 0.02, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.816724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{'max_depth': 2, 'learning_rate': 0.02, 'min_samples_leaf': 30, 'subsample': 0.1}</td>\n",
       "      <td>0.816629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                 params  \\\n",
       "134  {'max_depth': 3, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.4}   \n",
       "113  {'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 20, 'subsample': 0.3}   \n",
       "118  {'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.3}   \n",
       "119  {'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.4}   \n",
       "59   {'max_depth': 1, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.4}    \n",
       "58   {'max_depth': 1, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.3}    \n",
       "82   {'max_depth': 3, 'learning_rate': 0.01, 'min_samples_leaf': 20, 'subsample': 0.2}    \n",
       "74   {'max_depth': 2, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.4}    \n",
       "88   {'max_depth': 3, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.3}    \n",
       "133  {'max_depth': 3, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.3}   \n",
       "..                                                                                  ...   \n",
       "31   {'max_depth': 3, 'learning_rate': 0.02, 'min_samples_leaf': 10, 'subsample': 0.15}   \n",
       "85   {'max_depth': 3, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.1}    \n",
       "130  {'max_depth': 3, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.1}   \n",
       "55   {'max_depth': 1, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.1}    \n",
       "115  {'max_depth': 2, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.1}   \n",
       "100  {'max_depth': 1, 'learning_rate': 0.005, 'min_samples_leaf': 30, 'subsample': 0.1}   \n",
       "40   {'max_depth': 3, 'learning_rate': 0.02, 'min_samples_leaf': 30, 'subsample': 0.1}    \n",
       "70   {'max_depth': 2, 'learning_rate': 0.01, 'min_samples_leaf': 30, 'subsample': 0.1}    \n",
       "10   {'max_depth': 1, 'learning_rate': 0.02, 'min_samples_leaf': 30, 'subsample': 0.1}    \n",
       "25   {'max_depth': 2, 'learning_rate': 0.02, 'min_samples_leaf': 30, 'subsample': 0.1}    \n",
       "\n",
       "     mean_test_score  \n",
       "134  0.850188         \n",
       "113  0.849860         \n",
       "118  0.849457         \n",
       "119  0.848894         \n",
       "59   0.848731         \n",
       "58   0.848631         \n",
       "82   0.847872         \n",
       "74   0.847684         \n",
       "88   0.847291         \n",
       "133  0.846632         \n",
       "..        ...         \n",
       "31   0.826461         \n",
       "85   0.820724         \n",
       "130  0.820555         \n",
       "55   0.820404         \n",
       "115  0.819148         \n",
       "100  0.818847         \n",
       "40   0.817413         \n",
       "70   0.817397         \n",
       "10   0.816724         \n",
       "25   0.816629         \n",
       "\n",
       "[135 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAocAAAJCCAYAAACyMW3lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNIcAAACw+38EMUno8vbhmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a13e4b320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = modules.get_ml_data(p222_1, if_remove_icd = 0, if_remove_sleep=1)\n",
    "parameter_tuning(p222_1,X_train, X_test, y_train, y_test,7, C_range_num = 100, \n",
    "                     nfold = 10, if_save = 0, if_show = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>region_start_time</th>\n",
       "      <th>sleep</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "      <th>theta1</th>\n",
       "      <th>theta2</th>\n",
       "      <th>theta3</th>\n",
       "      <th>...</th>\n",
       "      <th>all1</th>\n",
       "      <th>all2</th>\n",
       "      <th>all3</th>\n",
       "      <th>all4</th>\n",
       "      <th>i12</th>\n",
       "      <th>i34</th>\n",
       "      <th>epoch</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>if_stimulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1.309980e+17</td>\n",
       "      <td>2016-02-12 03:59:55.968000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.436218</td>\n",
       "      <td>364.151777</td>\n",
       "      <td>63.215202</td>\n",
       "      <td>202.719760</td>\n",
       "      <td>101.081960</td>\n",
       "      <td>344.757574</td>\n",
       "      <td>133.174825</td>\n",
       "      <td>...</td>\n",
       "      <td>393.474657</td>\n",
       "      <td>1179.278104</td>\n",
       "      <td>502.973709</td>\n",
       "      <td>872.480713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>1.309980e+17</td>\n",
       "      <td>2016-02-12 15:59:53.030400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.952992</td>\n",
       "      <td>206.077023</td>\n",
       "      <td>34.415236</td>\n",
       "      <td>91.723497</td>\n",
       "      <td>102.638616</td>\n",
       "      <td>395.431395</td>\n",
       "      <td>87.026204</td>\n",
       "      <td>...</td>\n",
       "      <td>288.098271</td>\n",
       "      <td>1034.826536</td>\n",
       "      <td>407.075884</td>\n",
       "      <td>805.376122</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1.309989e+17</td>\n",
       "      <td>2016-02-12 20:59:51.964800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.293362</td>\n",
       "      <td>244.848856</td>\n",
       "      <td>34.561427</td>\n",
       "      <td>66.295568</td>\n",
       "      <td>63.117262</td>\n",
       "      <td>448.411932</td>\n",
       "      <td>85.858412</td>\n",
       "      <td>...</td>\n",
       "      <td>214.307536</td>\n",
       "      <td>995.629253</td>\n",
       "      <td>477.849916</td>\n",
       "      <td>709.906735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>1.309989e+17</td>\n",
       "      <td>2016-02-13 03:59:41.020800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.736205</td>\n",
       "      <td>121.930635</td>\n",
       "      <td>23.548927</td>\n",
       "      <td>73.607882</td>\n",
       "      <td>58.570117</td>\n",
       "      <td>291.327793</td>\n",
       "      <td>73.162509</td>\n",
       "      <td>...</td>\n",
       "      <td>199.280474</td>\n",
       "      <td>765.134289</td>\n",
       "      <td>423.093376</td>\n",
       "      <td>693.146568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1.309989e+17</td>\n",
       "      <td>2016-02-13 15:59:39.033600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.837088</td>\n",
       "      <td>159.177819</td>\n",
       "      <td>29.398845</td>\n",
       "      <td>97.541387</td>\n",
       "      <td>87.276202</td>\n",
       "      <td>475.231517</td>\n",
       "      <td>75.751883</td>\n",
       "      <td>...</td>\n",
       "      <td>256.451487</td>\n",
       "      <td>1035.538565</td>\n",
       "      <td>379.213366</td>\n",
       "      <td>822.637315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>1.309997e+17</td>\n",
       "      <td>2016-02-13 20:59:37.968000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.313928</td>\n",
       "      <td>132.279432</td>\n",
       "      <td>17.281833</td>\n",
       "      <td>74.976220</td>\n",
       "      <td>68.591295</td>\n",
       "      <td>388.279137</td>\n",
       "      <td>68.100650</td>\n",
       "      <td>...</td>\n",
       "      <td>225.398151</td>\n",
       "      <td>858.877318</td>\n",
       "      <td>416.540262</td>\n",
       "      <td>790.444117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1.309997e+17</td>\n",
       "      <td>2016-02-14 03:59:36.960000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.166778</td>\n",
       "      <td>273.677298</td>\n",
       "      <td>33.567358</td>\n",
       "      <td>81.248635</td>\n",
       "      <td>67.960011</td>\n",
       "      <td>407.512272</td>\n",
       "      <td>63.612451</td>\n",
       "      <td>...</td>\n",
       "      <td>246.179542</td>\n",
       "      <td>1191.769707</td>\n",
       "      <td>405.645495</td>\n",
       "      <td>690.393928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>1.309997e+17</td>\n",
       "      <td>2016-02-14 15:59:34.022400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.434457</td>\n",
       "      <td>146.502404</td>\n",
       "      <td>26.545324</td>\n",
       "      <td>113.718263</td>\n",
       "      <td>92.329255</td>\n",
       "      <td>422.707738</td>\n",
       "      <td>83.928926</td>\n",
       "      <td>...</td>\n",
       "      <td>263.857999</td>\n",
       "      <td>988.330991</td>\n",
       "      <td>427.060643</td>\n",
       "      <td>896.721623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>1.310006e+17</td>\n",
       "      <td>2016-02-14 20:59:33.993600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.334224</td>\n",
       "      <td>375.483229</td>\n",
       "      <td>49.405315</td>\n",
       "      <td>121.113454</td>\n",
       "      <td>92.537941</td>\n",
       "      <td>457.397495</td>\n",
       "      <td>93.631180</td>\n",
       "      <td>...</td>\n",
       "      <td>364.616993</td>\n",
       "      <td>1360.944809</td>\n",
       "      <td>473.557109</td>\n",
       "      <td>802.123650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>1.310006e+17</td>\n",
       "      <td>2016-02-15 03:59:22.963200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.442024</td>\n",
       "      <td>143.936747</td>\n",
       "      <td>40.430945</td>\n",
       "      <td>101.728189</td>\n",
       "      <td>40.123491</td>\n",
       "      <td>232.779533</td>\n",
       "      <td>87.369548</td>\n",
       "      <td>...</td>\n",
       "      <td>193.653876</td>\n",
       "      <td>776.878225</td>\n",
       "      <td>372.658872</td>\n",
       "      <td>673.987872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>1.312097e+17</td>\n",
       "      <td>2016-10-13 20:52:13.958400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.849526</td>\n",
       "      <td>130.582828</td>\n",
       "      <td>21.782559</td>\n",
       "      <td>77.280066</td>\n",
       "      <td>85.145398</td>\n",
       "      <td>392.384598</td>\n",
       "      <td>109.818866</td>\n",
       "      <td>...</td>\n",
       "      <td>247.567088</td>\n",
       "      <td>922.099081</td>\n",
       "      <td>606.518352</td>\n",
       "      <td>810.325312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>1.312097e+17</td>\n",
       "      <td>2016-10-14 03:52:01.977600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.480218</td>\n",
       "      <td>166.409104</td>\n",
       "      <td>38.253607</td>\n",
       "      <td>86.272224</td>\n",
       "      <td>73.847559</td>\n",
       "      <td>328.566468</td>\n",
       "      <td>76.675459</td>\n",
       "      <td>...</td>\n",
       "      <td>242.849650</td>\n",
       "      <td>1057.874291</td>\n",
       "      <td>429.902087</td>\n",
       "      <td>733.484519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>1.312097e+17</td>\n",
       "      <td>2016-10-14 15:51:59.990400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.472554</td>\n",
       "      <td>121.919630</td>\n",
       "      <td>32.526010</td>\n",
       "      <td>87.067608</td>\n",
       "      <td>100.492996</td>\n",
       "      <td>349.743992</td>\n",
       "      <td>76.188373</td>\n",
       "      <td>...</td>\n",
       "      <td>290.464335</td>\n",
       "      <td>963.307718</td>\n",
       "      <td>422.321791</td>\n",
       "      <td>761.239581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>1.312106e+17</td>\n",
       "      <td>2016-10-14 20:51:57.974400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.956015</td>\n",
       "      <td>122.934611</td>\n",
       "      <td>22.868977</td>\n",
       "      <td>65.386303</td>\n",
       "      <td>79.506908</td>\n",
       "      <td>321.541046</td>\n",
       "      <td>104.788589</td>\n",
       "      <td>...</td>\n",
       "      <td>240.579613</td>\n",
       "      <td>906.777277</td>\n",
       "      <td>475.430731</td>\n",
       "      <td>768.482556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>1.312106e+17</td>\n",
       "      <td>2016-10-15 03:51:56.966400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.483495</td>\n",
       "      <td>241.823225</td>\n",
       "      <td>84.759599</td>\n",
       "      <td>171.891446</td>\n",
       "      <td>69.457314</td>\n",
       "      <td>367.642559</td>\n",
       "      <td>156.888956</td>\n",
       "      <td>...</td>\n",
       "      <td>282.585202</td>\n",
       "      <td>1189.297221</td>\n",
       "      <td>672.973876</td>\n",
       "      <td>985.201459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>1.312106e+17</td>\n",
       "      <td>2016-10-15 15:51:54.979200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.213590</td>\n",
       "      <td>110.498612</td>\n",
       "      <td>29.145385</td>\n",
       "      <td>77.395535</td>\n",
       "      <td>97.463437</td>\n",
       "      <td>359.090868</td>\n",
       "      <td>87.032101</td>\n",
       "      <td>...</td>\n",
       "      <td>259.456104</td>\n",
       "      <td>979.014053</td>\n",
       "      <td>411.963965</td>\n",
       "      <td>859.602739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>1.312114e+17</td>\n",
       "      <td>2016-10-15 20:51:54.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.787801</td>\n",
       "      <td>104.188756</td>\n",
       "      <td>21.657637</td>\n",
       "      <td>75.277413</td>\n",
       "      <td>75.386855</td>\n",
       "      <td>304.362008</td>\n",
       "      <td>89.117432</td>\n",
       "      <td>...</td>\n",
       "      <td>250.096486</td>\n",
       "      <td>822.743905</td>\n",
       "      <td>496.960676</td>\n",
       "      <td>867.736286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>1.312114e+17</td>\n",
       "      <td>2016-10-16 03:51:42.969600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>116.646185</td>\n",
       "      <td>585.714496</td>\n",
       "      <td>118.107709</td>\n",
       "      <td>291.785970</td>\n",
       "      <td>127.110164</td>\n",
       "      <td>494.224383</td>\n",
       "      <td>160.089297</td>\n",
       "      <td>...</td>\n",
       "      <td>392.468064</td>\n",
       "      <td>1609.738531</td>\n",
       "      <td>585.400155</td>\n",
       "      <td>1150.547519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>1.312114e+17</td>\n",
       "      <td>2016-10-16 15:51:40.032000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.538403</td>\n",
       "      <td>373.470750</td>\n",
       "      <td>43.846616</td>\n",
       "      <td>100.506508</td>\n",
       "      <td>148.405154</td>\n",
       "      <td>378.227376</td>\n",
       "      <td>92.554714</td>\n",
       "      <td>...</td>\n",
       "      <td>470.190253</td>\n",
       "      <td>1369.334836</td>\n",
       "      <td>625.358306</td>\n",
       "      <td>1041.620044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>1.312122e+17</td>\n",
       "      <td>2016-10-16 20:51:38.966400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.168131</td>\n",
       "      <td>104.536728</td>\n",
       "      <td>17.215758</td>\n",
       "      <td>73.935835</td>\n",
       "      <td>61.085664</td>\n",
       "      <td>290.991518</td>\n",
       "      <td>104.835974</td>\n",
       "      <td>...</td>\n",
       "      <td>206.057316</td>\n",
       "      <td>790.201809</td>\n",
       "      <td>539.955091</td>\n",
       "      <td>828.139389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>222_1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>728 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename          region_start_time  sleep      delta1      delta2  \\\n",
       "521   1.309980e+17 2016-02-12 03:59:55.968000  1.0    65.436218   364.151777   \n",
       "522   1.309980e+17 2016-02-12 15:59:53.030400  0.0    66.952992   206.077023   \n",
       "523   1.309989e+17 2016-02-12 20:59:51.964800  0.0    39.293362   244.848856   \n",
       "524   1.309989e+17 2016-02-13 03:59:41.020800  1.0    37.736205   121.930635   \n",
       "525   1.309989e+17 2016-02-13 15:59:39.033600  0.0    48.837088   159.177819   \n",
       "526   1.309997e+17 2016-02-13 20:59:37.968000  0.0    41.313928   132.279432   \n",
       "527   1.309997e+17 2016-02-14 03:59:36.960000  1.0    61.166778   273.677298   \n",
       "528   1.309997e+17 2016-02-14 15:59:34.022400  0.0    56.434457   146.502404   \n",
       "529   1.310006e+17 2016-02-14 20:59:33.993600  0.0    78.334224   375.483229   \n",
       "530   1.310006e+17 2016-02-15 03:59:22.963200  1.0    50.442024   143.936747   \n",
       "...            ...                        ...  ...          ...          ...   \n",
       "1239  1.312097e+17 2016-10-13 20:52:13.958400  0.0    41.849526   130.582828   \n",
       "1240  1.312097e+17 2016-10-14 03:52:01.977600  1.0    45.480218   166.409104   \n",
       "1241  1.312097e+17 2016-10-14 15:51:59.990400  0.0    55.472554   121.919630   \n",
       "1242  1.312106e+17 2016-10-14 20:51:57.974400  0.0    40.956015   122.934611   \n",
       "1243  1.312106e+17 2016-10-15 03:51:56.966400  1.0    63.483495   241.823225   \n",
       "1244  1.312106e+17 2016-10-15 15:51:54.979200  0.0    44.213590   110.498612   \n",
       "1245  1.312114e+17 2016-10-15 20:51:54.000000  0.0    37.787801   104.188756   \n",
       "1246  1.312114e+17 2016-10-16 03:51:42.969600  1.0    116.646185  585.714496   \n",
       "1247  1.312114e+17 2016-10-16 15:51:40.032000  0.0    73.538403   373.470750   \n",
       "1248  1.312122e+17 2016-10-16 20:51:38.966400  0.0    38.168131   104.536728   \n",
       "\n",
       "          delta3      delta4      theta1      theta2      theta3  \\\n",
       "521   63.215202   202.719760  101.081960  344.757574  133.174825   \n",
       "522   34.415236   91.723497   102.638616  395.431395  87.026204    \n",
       "523   34.561427   66.295568   63.117262   448.411932  85.858412    \n",
       "524   23.548927   73.607882   58.570117   291.327793  73.162509    \n",
       "525   29.398845   97.541387   87.276202   475.231517  75.751883    \n",
       "526   17.281833   74.976220   68.591295   388.279137  68.100650    \n",
       "527   33.567358   81.248635   67.960011   407.512272  63.612451    \n",
       "528   26.545324   113.718263  92.329255   422.707738  83.928926    \n",
       "529   49.405315   121.113454  92.537941   457.397495  93.631180    \n",
       "530   40.430945   101.728189  40.123491   232.779533  87.369548    \n",
       "...         ...          ...        ...          ...        ...    \n",
       "1239  21.782559   77.280066   85.145398   392.384598  109.818866   \n",
       "1240  38.253607   86.272224   73.847559   328.566468  76.675459    \n",
       "1241  32.526010   87.067608   100.492996  349.743992  76.188373    \n",
       "1242  22.868977   65.386303   79.506908   321.541046  104.788589   \n",
       "1243  84.759599   171.891446  69.457314   367.642559  156.888956   \n",
       "1244  29.145385   77.395535   97.463437   359.090868  87.032101    \n",
       "1245  21.657637   75.277413   75.386855   304.362008  89.117432    \n",
       "1246  118.107709  291.785970  127.110164  494.224383  160.089297   \n",
       "1247  43.846616   100.506508  148.405154  378.227376  92.554714    \n",
       "1248  17.215758   73.935835   61.085664   290.991518  104.835974   \n",
       "\n",
       "          ...              all1         all2        all3         all4  i12  \\\n",
       "521       ...        393.474657  1179.278104  502.973709  872.480713   0.0   \n",
       "522       ...        288.098271  1034.826536  407.075884  805.376122   1.0   \n",
       "523       ...        214.307536  995.629253   477.849916  709.906735   0.0   \n",
       "524       ...        199.280474  765.134289   423.093376  693.146568   0.0   \n",
       "525       ...        256.451487  1035.538565  379.213366  822.637315   0.0   \n",
       "526       ...        225.398151  858.877318   416.540262  790.444117   0.0   \n",
       "527       ...        246.179542  1191.769707  405.645495  690.393928   0.0   \n",
       "528       ...        263.857999  988.330991   427.060643  896.721623   1.0   \n",
       "529       ...        364.616993  1360.944809  473.557109  802.123650   0.0   \n",
       "530       ...        193.653876  776.878225   372.658872  673.987872   0.0   \n",
       "...       ...               ...         ...          ...         ...   ...   \n",
       "1239      ...        247.567088  922.099081   606.518352  810.325312   0.0   \n",
       "1240      ...        242.849650  1057.874291  429.902087  733.484519   0.0   \n",
       "1241      ...        290.464335  963.307718   422.321791  761.239581   0.0   \n",
       "1242      ...        240.579613  906.777277   475.430731  768.482556   0.0   \n",
       "1243      ...        282.585202  1189.297221  672.973876  985.201459   0.0   \n",
       "1244      ...        259.456104  979.014053   411.963965  859.602739   0.0   \n",
       "1245      ...        250.096486  822.743905   496.960676  867.736286   0.0   \n",
       "1246      ...        392.468064  1609.738531  585.400155  1150.547519  0.0   \n",
       "1247      ...        470.190253  1369.334836  625.358306  1041.620044  0.0   \n",
       "1248      ...        206.057316  790.201809   539.955091  828.139389   0.0   \n",
       "\n",
       "      i34  epoch  label     id  if_stimulated  \n",
       "521   0.0  0      True   222_1  False          \n",
       "522   0.0  0      True   222_1  False          \n",
       "523   0.0  0      True   222_1  False          \n",
       "524   0.0  0      True   222_1  False          \n",
       "525   0.0  0      True   222_1  False          \n",
       "526   1.0  0      True   222_1  False          \n",
       "527   0.0  0      True   222_1  False          \n",
       "528   1.0  0      True   222_1  False          \n",
       "529   0.0  0      True   222_1  False          \n",
       "530   0.0  0      True   222_1  False          \n",
       "...   ... ..       ...     ...    ...          \n",
       "1239  1.0  7      False  222_1  False          \n",
       "1240  0.0  7      False  222_1  False          \n",
       "1241  0.0  7      False  222_1  False          \n",
       "1242  1.0  7      False  222_1  False          \n",
       "1243  0.0  7      False  222_1  False          \n",
       "1244  0.0  7      False  222_1  False          \n",
       "1245  0.0  7      False  222_1  False          \n",
       "1246  0.0  7      False  222_1  False          \n",
       "1247  0.0  7      False  222_1  False          \n",
       "1248  0.0  7      False  222_1  False          \n",
       "\n",
       "[728 rows x 37 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p222_1.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_tuning_sleep(pat, C_range_num, if_scaler = 1, if_remove_icd = 1, if_remove_sleep=1, sleep_class = None, if_save = 1, if_show = 0):\n",
    "    X_train, X_test, y_train, y_test = modules.get_ml_data(pat, if_scaler = if_scaler, if_remove_icd = if_remove_icd, if_remove_sleep = if_remove_sleep, sleep_class = sleep_class)\n",
    "    for classifier_int in tqdm.trange(1,hp.num_classifier + 1):\n",
    "        parameter_tuning(pat, X_train, X_test, y_train, y_test, C_range_num = C_range_num, classifier = classifier_int, if_save = if_save, if_show = if_show)\n",
    "    if if_save:\n",
    "        JJ.save_object(pat, hp.prepath_pat + pat.id +'_trained.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:   11.6s finished\n",
      "\n",
      " 14%|█▍        | 1/7 [00:11<01:09, 11.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6607538225646696\n",
      "Best parameters: {'C': 0.1858878725433549, 'penalty': 'l1'}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   26.6s finished\n",
      "\n",
      " 29%|██▊       | 2/7 [00:38<01:35, 19.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6866631208854427\n",
      "Best parameters: {'C': 5.005967430237205, 'gamma': 0.03571428571428571, 'kernel': 'rbf'}\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.5s finished\n",
      "\n",
      " 71%|███████▏  | 5/7 [00:41<00:16,  8.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6222094257168216\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 5}\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  5.1min finished\n",
      "\n",
      " 86%|████████▌ | 6/7 [05:50<00:58, 58.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.667040573640753\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 16, 'min_samples_split': 20, 'max_features': 'auto'}\n",
      "Fitting 10 folds for each of 135 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1350 out of 1350 | elapsed: 26.6min finished\n",
      "\n",
      "100%|██████████| 7/7 [32:26<00:00, 278.12s/it]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6534838697546004\n",
      "Best parameters: {'max_depth': 1, 'learning_rate': 0.02, 'min_samples_leaf': 30, 'subsample': 0.2}\n"
     ]
    }
   ],
   "source": [
    "parameter_tuning_sleep(p222_1, C_range_num = 100, if_remove_sleep = 1,sleep_class =None, if_save = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_tuning_all(pat, C_range_num, if_scaler = 1, if_remove_icd = 1, if_remove_sleep=1, if_save = 1, if_show = 0):\n",
    "    X_train, X_test, y_train, y_test = modules.get_ml_data(pat, if_scaler = if_scaler, if_remove_icd = if_remove_icd, if_remove_sleep = if_remove_sleep)\n",
    "    for classifier_int in tqdm.trange(1,hp.num_classifier + 1):\n",
    "        parameter_tuning(pat, X_train, X_test, y_train, y_test, C_range_num = C_range_num, classifier = classifier_int, if_save = if_save, if_show = if_show)\n",
    "    if if_save:\n",
    "        JJ.save_object(pat, hp.prepath_pat + pat.id +'_trained_sleep.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:   12.3s finished\n",
      "\r",
      " 14%|█▍        | 1/7 [00:12<01:13, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6601387357774628\n",
      "Best parameters: {'C': 0.18151583967130078, 'penalty': 'l1'}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   26.3s finished\n",
      " 57%|█████▋    | 4/7 [00:38<00:29,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6865372921513665\n",
      "Best parameters: {'C': 5.207792123899966, 'gamma': 0.03571428571428571, 'kernel': 'rbf'}\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.4s finished\n",
      "\r",
      " 71%|███████▏  | 5/7 [00:42<00:16,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6314990319360557\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 10}\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  5.0min finished\n",
      " 86%|████████▌ | 6/7 [05:45<00:57, 57.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6695946333440731\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 15, 'min_samples_split': 25, 'max_features': 'auto'}\n",
      "Fitting 10 folds for each of 135 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1350 out of 1350 | elapsed: 26.5min finished\n",
      "100%|██████████| 7/7 [32:16<00:00, 276.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6561958743715802\n",
      "Best parameters: {'max_depth': 2, 'learning_rate': 0.01, 'min_samples_leaf': 20, 'subsample': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameter_tuning_all(p222_1, C_range_num = 100, if_scaler = hp.if_scaler, \n",
    "                     if_remove_icd = hp.if_remove_icd, if_remove_sleep = 1, if_save = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8603351955307262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "JJ.ensemble_model(X_train, y_train, X_test, y_test, p231, if_save = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outliers removed: 19\n",
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:   11.9s finished\n",
      "\r",
      " 14%|█▍        | 1/7 [00:11<01:11, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6607585081923943\n",
      "Best parameters: {'C': 0.19517445578083695, 'penalty': 'l1'}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   27.6s finished\n",
      "\r",
      " 29%|██▊       | 2/7 [00:39<01:38, 19.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6867889496195189\n",
      "Best parameters: {'C': 4.90241546749882, 'kernel': 'rbf', 'gamma': 0.03571428571428571}\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.6s finished\n",
      "\r",
      " 71%|███████▏  | 5/7 [00:43<00:17,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6311262313615518\n",
      "Best parameters: {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 8}\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  5.1min finished\n",
      " 86%|████████▌ | 6/7 [05:50<00:58, 58.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6664817143862415\n",
      "Best parameters: {'max_features': 'auto', 'min_samples_split': 30, 'criterion': 'entropy', 'max_depth': 16}\n",
      "Fitting 10 folds for each of 135 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1350 out of 1350 | elapsed: 61.2min finished\n",
      "100%|██████████| 7/7 [1:07:02<00:00, 574.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6507336969013348\n",
      "Best parameters: {'learning_rate': 0.005, 'min_samples_leaf': 20, 'subsample': 0.2, 'max_depth': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameter_tuning_all(p222_1, C_range_num = 100, if_scaler = hp.if_scaler, if_remove_icd = hp.if_remove_icd, if_save = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
