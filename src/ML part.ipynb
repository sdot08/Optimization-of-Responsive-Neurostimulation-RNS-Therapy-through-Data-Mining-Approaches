{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/hp/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import operator\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from random import shuffle\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "#PLOT CONFUSION MATRIX\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import pickle\n",
    "#matrix inverse\n",
    "from numpy.linalg import inv\n",
    "import jj_basic_fn as JJ\n",
    "from hyperparams import Hyperparams as hp\n",
    "from patient import patient\n",
    "import prep\n",
    "import plot_funcs\n",
    "import modules\n",
    "#default size of the graph\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "n_classifier = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>region_start_time</th>\n",
       "      <th>sleep</th>\n",
       "      <th>delta1</th>\n",
       "      <th>delta2</th>\n",
       "      <th>delta3</th>\n",
       "      <th>delta4</th>\n",
       "      <th>theta1</th>\n",
       "      <th>theta2</th>\n",
       "      <th>theta3</th>\n",
       "      <th>...</th>\n",
       "      <th>all1</th>\n",
       "      <th>all2</th>\n",
       "      <th>all3</th>\n",
       "      <th>all4</th>\n",
       "      <th>i12</th>\n",
       "      <th>i34</th>\n",
       "      <th>epoch</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>if_stimulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>1.313099e+17</td>\n",
       "      <td>2017-02-07 04:00:29.000016000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.540879</td>\n",
       "      <td>43.652166</td>\n",
       "      <td>169.626561</td>\n",
       "      <td>47.994966</td>\n",
       "      <td>77.653426</td>\n",
       "      <td>73.585284</td>\n",
       "      <td>132.283194</td>\n",
       "      <td>...</td>\n",
       "      <td>449.080826</td>\n",
       "      <td>497.912560</td>\n",
       "      <td>583.423582</td>\n",
       "      <td>228.331494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>1.313099e+17</td>\n",
       "      <td>2017-02-07 10:00:27.000028800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.371989</td>\n",
       "      <td>36.895066</td>\n",
       "      <td>99.998346</td>\n",
       "      <td>40.672797</td>\n",
       "      <td>83.025855</td>\n",
       "      <td>49.766390</td>\n",
       "      <td>101.685936</td>\n",
       "      <td>...</td>\n",
       "      <td>420.025064</td>\n",
       "      <td>374.851675</td>\n",
       "      <td>431.144569</td>\n",
       "      <td>200.212985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>1.313100e+17</td>\n",
       "      <td>2017-02-07 16:00:26.000035200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.857643</td>\n",
       "      <td>43.743936</td>\n",
       "      <td>131.445025</td>\n",
       "      <td>50.462590</td>\n",
       "      <td>78.433538</td>\n",
       "      <td>52.023979</td>\n",
       "      <td>110.203051</td>\n",
       "      <td>...</td>\n",
       "      <td>514.946178</td>\n",
       "      <td>386.780745</td>\n",
       "      <td>504.752669</td>\n",
       "      <td>212.883592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>1.313103e+17</td>\n",
       "      <td>2017-02-07 22:00:13.000032000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.677420</td>\n",
       "      <td>85.919986</td>\n",
       "      <td>104.240635</td>\n",
       "      <td>33.813202</td>\n",
       "      <td>111.814219</td>\n",
       "      <td>114.253805</td>\n",
       "      <td>110.936082</td>\n",
       "      <td>...</td>\n",
       "      <td>619.553782</td>\n",
       "      <td>619.705097</td>\n",
       "      <td>461.335672</td>\n",
       "      <td>202.953935</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>1.313103e+17</td>\n",
       "      <td>2017-02-08 04:00:12.000038400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.679727</td>\n",
       "      <td>60.534557</td>\n",
       "      <td>157.204467</td>\n",
       "      <td>58.719918</td>\n",
       "      <td>79.233067</td>\n",
       "      <td>64.724569</td>\n",
       "      <td>102.978850</td>\n",
       "      <td>...</td>\n",
       "      <td>449.277355</td>\n",
       "      <td>418.643967</td>\n",
       "      <td>507.236063</td>\n",
       "      <td>213.607815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>1.313108e+17</td>\n",
       "      <td>2017-02-08 10:00:10.999958400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.925373</td>\n",
       "      <td>54.099957</td>\n",
       "      <td>151.459489</td>\n",
       "      <td>57.000622</td>\n",
       "      <td>94.976730</td>\n",
       "      <td>44.356892</td>\n",
       "      <td>127.877129</td>\n",
       "      <td>...</td>\n",
       "      <td>538.850676</td>\n",
       "      <td>375.030443</td>\n",
       "      <td>544.170195</td>\n",
       "      <td>214.113030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>1.313108e+17</td>\n",
       "      <td>2017-02-08 16:00:09.999964800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.494190</td>\n",
       "      <td>50.823708</td>\n",
       "      <td>146.509136</td>\n",
       "      <td>54.689405</td>\n",
       "      <td>89.035278</td>\n",
       "      <td>65.329579</td>\n",
       "      <td>116.948584</td>\n",
       "      <td>...</td>\n",
       "      <td>488.830085</td>\n",
       "      <td>399.226190</td>\n",
       "      <td>526.187100</td>\n",
       "      <td>214.809479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>1.313117e+17</td>\n",
       "      <td>2017-02-09 15:59:54.999974400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.446978</td>\n",
       "      <td>40.611951</td>\n",
       "      <td>119.250325</td>\n",
       "      <td>55.420373</td>\n",
       "      <td>95.763307</td>\n",
       "      <td>71.407570</td>\n",
       "      <td>122.993170</td>\n",
       "      <td>...</td>\n",
       "      <td>493.822944</td>\n",
       "      <td>414.372099</td>\n",
       "      <td>487.273913</td>\n",
       "      <td>245.373050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>1.313126e+17</td>\n",
       "      <td>2017-02-09 21:59:42.999964800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>326.016488</td>\n",
       "      <td>256.695094</td>\n",
       "      <td>346.094522</td>\n",
       "      <td>96.836384</td>\n",
       "      <td>305.556517</td>\n",
       "      <td>262.238440</td>\n",
       "      <td>397.426399</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.509263</td>\n",
       "      <td>908.738281</td>\n",
       "      <td>1280.147218</td>\n",
       "      <td>938.436500</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>1.313126e+17</td>\n",
       "      <td>2017-02-10 03:59:41.999971200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.776059</td>\n",
       "      <td>51.659472</td>\n",
       "      <td>142.023056</td>\n",
       "      <td>55.387199</td>\n",
       "      <td>98.074770</td>\n",
       "      <td>73.821734</td>\n",
       "      <td>160.924270</td>\n",
       "      <td>...</td>\n",
       "      <td>455.327692</td>\n",
       "      <td>405.171813</td>\n",
       "      <td>540.379631</td>\n",
       "      <td>226.173082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>1.316262e+17</td>\n",
       "      <td>2018-02-08 15:52:23.000016000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.243438</td>\n",
       "      <td>43.210239</td>\n",
       "      <td>81.948003</td>\n",
       "      <td>42.546477</td>\n",
       "      <td>86.987421</td>\n",
       "      <td>45.853903</td>\n",
       "      <td>91.946623</td>\n",
       "      <td>...</td>\n",
       "      <td>487.394531</td>\n",
       "      <td>378.663167</td>\n",
       "      <td>433.764279</td>\n",
       "      <td>225.820331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>1.316280e+17</td>\n",
       "      <td>2018-02-10 15:51:53.000035200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>564.877490</td>\n",
       "      <td>283.476285</td>\n",
       "      <td>308.109692</td>\n",
       "      <td>142.293355</td>\n",
       "      <td>665.666407</td>\n",
       "      <td>483.709974</td>\n",
       "      <td>322.368834</td>\n",
       "      <td>...</td>\n",
       "      <td>2815.505281</td>\n",
       "      <td>1768.978821</td>\n",
       "      <td>1234.631765</td>\n",
       "      <td>922.408419</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>1.316288e+17</td>\n",
       "      <td>2018-02-10 21:51:40.000032000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.102750</td>\n",
       "      <td>69.435131</td>\n",
       "      <td>62.984487</td>\n",
       "      <td>41.659497</td>\n",
       "      <td>138.874609</td>\n",
       "      <td>122.032582</td>\n",
       "      <td>112.255994</td>\n",
       "      <td>...</td>\n",
       "      <td>802.851977</td>\n",
       "      <td>713.615109</td>\n",
       "      <td>420.858416</td>\n",
       "      <td>232.911835</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>1.316288e+17</td>\n",
       "      <td>2018-02-11 03:51:39.000038400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.911227</td>\n",
       "      <td>71.288475</td>\n",
       "      <td>166.739470</td>\n",
       "      <td>67.829428</td>\n",
       "      <td>166.410791</td>\n",
       "      <td>154.349315</td>\n",
       "      <td>182.929122</td>\n",
       "      <td>...</td>\n",
       "      <td>1036.873811</td>\n",
       "      <td>836.650797</td>\n",
       "      <td>688.216777</td>\n",
       "      <td>342.449182</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>1.316288e+17</td>\n",
       "      <td>2018-02-11 09:51:36.999964800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.774684</td>\n",
       "      <td>40.363361</td>\n",
       "      <td>101.068475</td>\n",
       "      <td>50.437324</td>\n",
       "      <td>119.974135</td>\n",
       "      <td>71.017368</td>\n",
       "      <td>162.974574</td>\n",
       "      <td>...</td>\n",
       "      <td>529.095024</td>\n",
       "      <td>417.060695</td>\n",
       "      <td>508.717931</td>\n",
       "      <td>234.859843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>1.316288e+17</td>\n",
       "      <td>2018-02-11 15:51:35.999971200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.142656</td>\n",
       "      <td>43.925946</td>\n",
       "      <td>121.402267</td>\n",
       "      <td>44.771501</td>\n",
       "      <td>100.667476</td>\n",
       "      <td>52.816564</td>\n",
       "      <td>128.193040</td>\n",
       "      <td>...</td>\n",
       "      <td>556.439138</td>\n",
       "      <td>424.977643</td>\n",
       "      <td>556.370841</td>\n",
       "      <td>310.321027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>1.316296e+17</td>\n",
       "      <td>2018-02-11 21:51:25.000041600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.162000</td>\n",
       "      <td>50.395396</td>\n",
       "      <td>91.166914</td>\n",
       "      <td>40.079455</td>\n",
       "      <td>89.966879</td>\n",
       "      <td>90.940386</td>\n",
       "      <td>96.887581</td>\n",
       "      <td>...</td>\n",
       "      <td>545.054194</td>\n",
       "      <td>512.572809</td>\n",
       "      <td>468.425396</td>\n",
       "      <td>194.634692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>1.316296e+17</td>\n",
       "      <td>2018-02-12 03:51:22.999968000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.536331</td>\n",
       "      <td>153.708886</td>\n",
       "      <td>189.820605</td>\n",
       "      <td>71.536294</td>\n",
       "      <td>225.670238</td>\n",
       "      <td>264.015213</td>\n",
       "      <td>207.829890</td>\n",
       "      <td>...</td>\n",
       "      <td>1154.645685</td>\n",
       "      <td>957.462098</td>\n",
       "      <td>744.384485</td>\n",
       "      <td>552.027510</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>1.316296e+17</td>\n",
       "      <td>2018-02-12 09:51:21.999974400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.753303</td>\n",
       "      <td>34.006378</td>\n",
       "      <td>103.498303</td>\n",
       "      <td>41.858699</td>\n",
       "      <td>113.615081</td>\n",
       "      <td>76.859677</td>\n",
       "      <td>149.850580</td>\n",
       "      <td>...</td>\n",
       "      <td>531.141503</td>\n",
       "      <td>450.546214</td>\n",
       "      <td>534.260576</td>\n",
       "      <td>231.510968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>1.316296e+17</td>\n",
       "      <td>2018-02-12 15:51:20.999980800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.690558</td>\n",
       "      <td>35.500397</td>\n",
       "      <td>83.216243</td>\n",
       "      <td>33.193961</td>\n",
       "      <td>84.812841</td>\n",
       "      <td>51.472621</td>\n",
       "      <td>88.715493</td>\n",
       "      <td>...</td>\n",
       "      <td>500.431814</td>\n",
       "      <td>429.036342</td>\n",
       "      <td>451.483614</td>\n",
       "      <td>207.038228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>893 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename             region_start_time  sleep      delta1  \\\n",
       "647   1.313099e+17 2017-02-07 04:00:29.000016000  1.0    52.540879    \n",
       "648   1.313099e+17 2017-02-07 10:00:27.000028800  0.0    67.371989    \n",
       "649   1.313100e+17 2017-02-07 16:00:26.000035200  0.0    99.857643    \n",
       "650   1.313103e+17 2017-02-07 22:00:13.000032000  0.0    129.677420   \n",
       "651   1.313103e+17 2017-02-08 04:00:12.000038400  1.0    91.679727    \n",
       "652   1.313108e+17 2017-02-08 10:00:10.999958400  0.0    116.925373   \n",
       "653   1.313108e+17 2017-02-08 16:00:09.999964800  0.0    89.494190    \n",
       "654   1.313117e+17 2017-02-09 15:59:54.999974400  0.0    82.446978    \n",
       "655   1.313126e+17 2017-02-09 21:59:42.999964800  0.0    326.016488   \n",
       "656   1.313126e+17 2017-02-10 03:59:41.999971200  1.0    72.776059    \n",
       "...            ...                           ...  ...          ...    \n",
       "1530  1.316262e+17 2018-02-08 15:52:23.000016000  0.0    92.243438    \n",
       "1531  1.316280e+17 2018-02-10 15:51:53.000035200  0.0    564.877490   \n",
       "1532  1.316288e+17 2018-02-10 21:51:40.000032000  0.0    128.102750   \n",
       "1533  1.316288e+17 2018-02-11 03:51:39.000038400  1.0    165.911227   \n",
       "1534  1.316288e+17 2018-02-11 09:51:36.999964800  0.0    97.774684    \n",
       "1535  1.316288e+17 2018-02-11 15:51:35.999971200  0.0    104.142656   \n",
       "1536  1.316296e+17 2018-02-11 21:51:25.000041600  0.0    113.162000   \n",
       "1537  1.316296e+17 2018-02-12 03:51:22.999968000  1.0    225.536331   \n",
       "1538  1.316296e+17 2018-02-12 09:51:21.999974400  0.0    85.753303    \n",
       "1539  1.316296e+17 2018-02-12 15:51:20.999980800  0.0    78.690558    \n",
       "\n",
       "          delta2      delta3      delta4      theta1      theta2      theta3  \\\n",
       "647   43.652166   169.626561  47.994966   77.653426   73.585284   132.283194   \n",
       "648   36.895066   99.998346   40.672797   83.025855   49.766390   101.685936   \n",
       "649   43.743936   131.445025  50.462590   78.433538   52.023979   110.203051   \n",
       "650   85.919986   104.240635  33.813202   111.814219  114.253805  110.936082   \n",
       "651   60.534557   157.204467  58.719918   79.233067   64.724569   102.978850   \n",
       "652   54.099957   151.459489  57.000622   94.976730   44.356892   127.877129   \n",
       "653   50.823708   146.509136  54.689405   89.035278   65.329579   116.948584   \n",
       "654   40.611951   119.250325  55.420373   95.763307   71.407570   122.993170   \n",
       "655   256.695094  346.094522  96.836384   305.556517  262.238440  397.426399   \n",
       "656   51.659472   142.023056  55.387199   98.074770   73.821734   160.924270   \n",
       "...         ...          ...        ...         ...         ...          ...   \n",
       "1530  43.210239   81.948003   42.546477   86.987421   45.853903   91.946623    \n",
       "1531  283.476285  308.109692  142.293355  665.666407  483.709974  322.368834   \n",
       "1532  69.435131   62.984487   41.659497   138.874609  122.032582  112.255994   \n",
       "1533  71.288475   166.739470  67.829428   166.410791  154.349315  182.929122   \n",
       "1534  40.363361   101.068475  50.437324   119.974135  71.017368   162.974574   \n",
       "1535  43.925946   121.402267  44.771501   100.667476  52.816564   128.193040   \n",
       "1536  50.395396   91.166914   40.079455   89.966879   90.940386   96.887581    \n",
       "1537  153.708886  189.820605  71.536294   225.670238  264.015213  207.829890   \n",
       "1538  34.006378   103.498303  41.858699   113.615081  76.859677   149.850580   \n",
       "1539  35.500397   83.216243   33.193961   84.812841   51.472621   88.715493    \n",
       "\n",
       "          ...               all1         all2         all3        all4   i12  \\\n",
       "647       ...        449.080826   497.912560   583.423582   228.331494  1.0    \n",
       "648       ...        420.025064   374.851675   431.144569   200.212985  0.0    \n",
       "649       ...        514.946178   386.780745   504.752669   212.883592  0.0    \n",
       "650       ...        619.553782   619.705097   461.335672   202.953935  4.0    \n",
       "651       ...        449.277355   418.643967   507.236063   213.607815  0.0    \n",
       "652       ...        538.850676   375.030443   544.170195   214.113030  0.0    \n",
       "653       ...        488.830085   399.226190   526.187100   214.809479  0.0    \n",
       "654       ...        493.822944   414.372099   487.273913   245.373050  0.0    \n",
       "655       ...        1013.509263  908.738281   1280.147218  938.436500  9.0    \n",
       "656       ...        455.327692   405.171813   540.379631   226.173082  0.0    \n",
       "...       ...               ...          ...          ...          ...  ...    \n",
       "1530      ...        487.394531   378.663167   433.764279   225.820331  0.0    \n",
       "1531      ...        2815.505281  1768.978821  1234.631765  922.408419  13.0   \n",
       "1532      ...        802.851977   713.615109   420.858416   232.911835  8.0    \n",
       "1533      ...        1036.873811  836.650797   688.216777   342.449182  4.0    \n",
       "1534      ...        529.095024   417.060695   508.717931   234.859843  0.0    \n",
       "1535      ...        556.439138   424.977643   556.370841   310.321027  0.0    \n",
       "1536      ...        545.054194   512.572809   468.425396   194.634692  1.0    \n",
       "1537      ...        1154.645685  957.462098   744.384485   552.027510  5.0    \n",
       "1538      ...        531.141503   450.546214   534.260576   231.510968  0.0    \n",
       "1539      ...        500.431814   429.036342   451.483614   207.038228  0.0    \n",
       "\n",
       "       i34  epoch  label   id  if_stimulated  \n",
       "647   0.0   0      True   231  False          \n",
       "648   0.0   0      True   231  False          \n",
       "649   1.0   0      True   231  False          \n",
       "650   2.0   0      True   231  False          \n",
       "651   0.0   0      True   231  False          \n",
       "652   1.0   0      True   231  False          \n",
       "653   0.0   0      True   231  False          \n",
       "654   0.0   0      True   231  False          \n",
       "655   28.0  0      True   231  False          \n",
       "656   0.0   0      True   231  False          \n",
       "...   ...  ..       ...   ...    ...          \n",
       "1530  0.0   11     False  231  False          \n",
       "1531  20.0  11     False  231  False          \n",
       "1532  1.0   11     False  231  False          \n",
       "1533  6.0   11     False  231  False          \n",
       "1534  0.0   11     False  231  False          \n",
       "1535  1.0   11     False  231  False          \n",
       "1536  1.0   11     False  231  False          \n",
       "1537  8.0   11     False  231  False          \n",
       "1538  0.0   11     False  231  False          \n",
       "1539  0.0   11     False  231  False          \n",
       "\n",
       "[893 rows x 37 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p231.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/Users/hp/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n"
     ]
    }
   ],
   "source": [
    "p231, p222_1, p222_2, p229 = modules.build_patients()\n",
    "pat_list = [p231, p222_1, p222_2, p222_3]\n",
    "for pat in pat_list:\n",
    "    JJ.save_object(pat, '../patients/' + pat.id +'.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_rs = hp.col_rs\n",
    "col_es = hp.col_es\n",
    "col_le = hp.col_le\n",
    "\n",
    "p231, p222_1, p222_2, p222_3, p229 = \\\n",
    "pickle.load(open(hp.prepath_pat + \"231.p\", \"rb\" )),\\\n",
    "pickle.load(open(hp.prepath_pat + \"222_1.p\", \"rb\" )), \\\n",
    "pickle.load(open(hp.prepath_pat + \"222_2.p\", \"rb\" )), \\\n",
    "pickle.load(open(hp.prepath_pat + \"222_3.p\", \"rb\" )),\\\n",
    "pickle.load(open(hp.prepath_pat + \"229.p\", \"rb\" ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_tuning(pat, X_train, X_test, y_train, y_test, classifier, C_range_num = 30, if_save = 0,\n",
    "                     nfold = 10, if_show = 1):\n",
    "    #defs is a dictionary to initiate classifier with the parameters that don't need to be tuned\n",
    "    defs = {}\n",
    "    defs['classifier'] = classifier\n",
    "    \n",
    "    num_instances, num_features = X_train.shape[0], X_train.shape[1]\n",
    "    n_fold = 10\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    CV = skf.split(np.zeros(len(y_train)), y_train)\n",
    "    \n",
    "\n",
    "    if classifier==1:\n",
    "        clf_name = 'Logistic Regression'\n",
    "        defs['class_weight'] = 'balanced'\n",
    "        defs['max_iter'] = 200\n",
    "        C_range = 10 ** np.random.uniform(-2, 1, size = C_range_num)\n",
    "        tuned_params = dict(penalty=['l1','l2'], C=C_range)\n",
    "    elif classifier == 2: \n",
    "        clf_name = 'SVM'\n",
    "        defs['class_weight'] = 'balanced'\n",
    "        kernel_list = ['rbf']    \n",
    "        gamma_list = [2**i*1/num_features for i in range(1)]\n",
    "        #degree_list = [2,3,4,5]\n",
    "        C_range = 10 ** np.random.uniform(-3, 1, size = C_range_num)\n",
    "        tuned_params = dict(kernel=kernel_list,gamma = gamma_list, C=C_range)\n",
    "\n",
    "    elif classifier==3:\n",
    "        clf_name = 'Gaussian Naive Bayes classifier'\n",
    "        prr = np.ones(2) * (1.0/2)\n",
    "        defs['priors'] = prr\n",
    "        clf_try = JJ.clf_list(defs)\n",
    "        clf_try.fit(X_train, y_train)\n",
    "        pat.estimator[classifier] = clf_try\n",
    "        return\n",
    "\n",
    "    elif classifier==4:\n",
    "        clf_name = 'Linear Discriminant Analysis'\n",
    "        prr = np.ones(2) * (1.0/2)\n",
    "        defs['solver'] = 'eigen'  # 'svd', 'lsqr', 'eigen'\n",
    "        defs['shrinkage'] = 'auto'\n",
    "        defs['priors'] = prr\n",
    "        clf_try = JJ.clf_list(defs)\n",
    "        clf_try.fit(X_train, y_train)\n",
    "        pat.estimator[classifier] = clf_try\n",
    "        return\n",
    "    elif classifier == 5:\n",
    "        clf_name = 'decision tree'\n",
    "        mss_list = [5,10,20,40,60]\n",
    "        criterion_list = ['entropy']\n",
    "        max_depth_list = [3,4,5,8,12,18]\n",
    "        clf_name = 'decision tree'\n",
    "        tuned_params = dict(criterion=criterion_list, min_samples_split=mss_list, max_depth = max_depth_list)\n",
    "    elif classifier == 6:\n",
    "        clf_name = 'random forest'\n",
    "        defs['n_estimators'] = 600\n",
    "        mss_list = [20,25,30,40]\n",
    "        criterion_list = ['entropy']\n",
    "        max_depth_list = [12,13,14,15,16]\n",
    "        max_features_list = ['auto']\n",
    "        tuned_params = dict(criterion=criterion_list, min_samples_split=mss_list, max_depth = max_depth_list, max_features = max_features_list)\n",
    "    elif classifier == 7:\n",
    "        clf_name = 'gradient boosting'\n",
    "        defs['n_estimators'] = 2000\n",
    "        max_depth_list, subsample_list, learning_rate_list, min_samples_leaf_list = [1,2,3], [0.1,0.15,0.2, 0.3, 0.4], [0.02, 0.01,0.005], [10,20,30] \n",
    "        #params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,\n",
    "        #  'learning_rate': 0.01, 'min_samples_leaf': 10, 'random_state': 3}\n",
    "        tuned_params = dict(max_depth=max_depth_list, subsample = subsample_list,learning_rate = learning_rate_list, min_samples_leaf= min_samples_leaf_list)\n",
    "    \n",
    "        \n",
    "    clf_try = JJ.clf_list(defs)\n",
    "    \n",
    "    clf_grid = GridSearchCV(clf_try,\n",
    "                            param_grid=tuned_params,\n",
    "                            cv=CV,\n",
    "                            scoring = 'roc_auc',\n",
    "                            verbose=1,\n",
    "                           return_train_score = True)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    print('Best score for validations set: {}'.format(clf_grid.best_score_))\n",
    "    print('Best parameters: {}'.format(clf_grid.best_params_))\n",
    "\n",
    "    clf_best = clf_grid.best_estimator_\n",
    "\n",
    "    y_pred = clf_best.predict(X_test)\n",
    "    df = pd.DataFrame(clf_grid.cv_results_)\n",
    "    if if_show:\n",
    "        JJ.show_result(y_pred, y_test, df, clf_name, if_save = if_save)\n",
    "    \n",
    "    if if_save:\n",
    "        pat.result[classifier] = df\n",
    "        pat.estimator[classifier] = clf_best\n",
    "        pat.score[classifier] = clf_grid.best_score_\n",
    "        pat.params[classifier] = clf_grid.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
      "Best score for validations set: 0.728700918964077\n",
      "Best parameters: {'penalty': 'l2', 'C': 0.06196918137944155}\n",
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:   12.9s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.06196918137944155}</td>\n",
       "      <td>0.728701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.06428409085929962}</td>\n",
       "      <td>0.728693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.08376818643305572}</td>\n",
       "      <td>0.728676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.08250564721819544}</td>\n",
       "      <td>0.728676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.08121988357466467}</td>\n",
       "      <td>0.728415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.07712476885084417}</td>\n",
       "      <td>0.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.08969076064936082}</td>\n",
       "      <td>0.728158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.06853552435886708}</td>\n",
       "      <td>0.728129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.06918594285670775}</td>\n",
       "      <td>0.727872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>{'penalty': 'l2', 'C': 0.0563957331324413}</td>\n",
       "      <td>0.727623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.014075317833727768}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.012656756687970125}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.012051840672665412}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.012042623704738412}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.01025450603233791}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.011993037672876536}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.014028174272223982}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.01852972943221925}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.015571883555248446}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>{'penalty': 'l1', 'C': 0.013024419166318283}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           params  mean_test_score\n",
       "97   {'penalty': 'l2', 'C': 0.06196918137944155}   0.728701       \n",
       "197  {'penalty': 'l2', 'C': 0.06428409085929962}   0.728693       \n",
       "145  {'penalty': 'l2', 'C': 0.08376818643305572}   0.728676       \n",
       "15   {'penalty': 'l2', 'C': 0.08250564721819544}   0.728676       \n",
       "63   {'penalty': 'l2', 'C': 0.08121988357466467}   0.728415       \n",
       "83   {'penalty': 'l2', 'C': 0.07712476885084417}   0.728400       \n",
       "105  {'penalty': 'l2', 'C': 0.08969076064936082}   0.728158       \n",
       "5    {'penalty': 'l2', 'C': 0.06853552435886708}   0.728129       \n",
       "43   {'penalty': 'l2', 'C': 0.06918594285670775}   0.727872       \n",
       "95   {'penalty': 'l2', 'C': 0.0563957331324413}    0.727623       \n",
       "..                                          ...         ...       \n",
       "198  {'penalty': 'l1', 'C': 0.014075317833727768}  0.500000       \n",
       "64   {'penalty': 'l1', 'C': 0.012656756687970125}  0.500000       \n",
       "10   {'penalty': 'l1', 'C': 0.012051840672665412}  0.500000       \n",
       "178  {'penalty': 'l1', 'C': 0.012042623704738412}  0.500000       \n",
       "98   {'penalty': 'l1', 'C': 0.01025450603233791}   0.500000       \n",
       "180  {'penalty': 'l1', 'C': 0.011993037672876536}  0.500000       \n",
       "12   {'penalty': 'l1', 'C': 0.014028174272223982}  0.500000       \n",
       "140  {'penalty': 'l1', 'C': 0.01852972943221925}   0.500000       \n",
       "90   {'penalty': 'l1', 'C': 0.015571883555248446}  0.500000       \n",
       "132  {'penalty': 'l1', 'C': 0.013024419166318283}  0.500000       \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAJCCAYAAABUGKahAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRhAAAMBH0QgCAAD4qP8Hw/Lh45Q7AAkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17be8828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = modules.get_ml_data(p229, if_remove_icd = 1, if_remove_sleep=1, if_remove_le=0, le_class = 1)\n",
    "parameter_tuning(p229,X_train, X_test, y_train, y_test,1, C_range_num = 100, \n",
    "                     nfold = 10, if_save = 0, if_show = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_tuning_all(pat, C_range_num, if_scaler = 1, if_remove_icd = 1, if_remove_sleep=1, if_remove_le = 1, le_class = None, if_save = 1, if_show = 0):\n",
    "    X_train, X_test, y_train, y_test = modules.get_ml_data(pat, if_scaler = if_scaler, if_remove_icd = if_remove_icd, if_remove_sleep = if_remove_sleep, if_remove_le = if_remove_le, le_class = le_class)\n",
    "    for classifier_int in tqdm.trange(1,hp.num_classifier + 1):\n",
    "        parameter_tuning(pat, X_train, X_test, y_train, y_test, C_range_num = C_range_num, classifier = classifier_int, if_save = if_save, if_show = if_show)\n",
    "    if if_save:\n",
    "        JJ.save_object(pat, hp.prepath_pat + pat.id +'_trained.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outliers removed: 24\n",
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:   18.4s finished\n",
      "\r",
      " 14%|█▍        | 1/7 [00:18<01:50, 18.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.8173735575999131\n",
      "Best parameters: {'penalty': 'l1', 'C': 0.47299083601862174}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   29.6s finished\n",
      "\r",
      " 29%|██▊       | 2/7 [00:48<02:00, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.8311342563579799\n",
      "Best parameters: {'gamma': 0.041666666666666664, 'kernel': 'rbf', 'C': 4.205710118374298}\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.7s finished\n",
      "\r",
      " 71%|███████▏  | 5/7 [00:51<00:20, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.7785411392140643\n",
      "Best parameters: {'max_depth': 5, 'criterion': 'entropy', 'min_samples_split': 10}\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  4.5min finished\n",
      " 86%|████████▌ | 6/7 [05:26<00:54, 54.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.8436788780357828\n",
      "Best parameters: {'max_depth': 16, 'criterion': 'entropy', 'max_features': 'auto', 'min_samples_split': 20}\n",
      "Fitting 10 folds for each of 135 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1350 out of 1350 | elapsed: 26.8min finished\n",
      "100%|██████████| 7/7 [32:15<00:00, 276.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.8508389107529309\n",
      "Best parameters: {'max_depth': 2, 'learning_rate': 0.005, 'subsample': 0.3, 'min_samples_leaf': 30}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameter_tuning_all(p231, C_range_num = 100, if_scaler = hp.if_scaler, \n",
    "                     if_remove_icd = hp.if_remove_icd, if_remove_sleep = 1, le_class = None, if_remove_le = 1, if_save = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8603351955307262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "JJ.ensemble_model(X_train, y_train, X_test, y_test, p231, if_save = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total outliers removed: 19\n",
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed:   11.9s finished\n",
      "\r",
      " 14%|█▍        | 1/7 [00:11<01:11, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6607585081923943\n",
      "Best parameters: {'C': 0.19517445578083695, 'penalty': 'l1'}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:   27.6s finished\n",
      "\r",
      " 29%|██▊       | 2/7 [00:39<01:38, 19.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6867889496195189\n",
      "Best parameters: {'C': 4.90241546749882, 'kernel': 'rbf', 'gamma': 0.03571428571428571}\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    3.6s finished\n",
      "\r",
      " 71%|███████▏  | 5/7 [00:43<00:17,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6311262313615518\n",
      "Best parameters: {'min_samples_split': 10, 'criterion': 'entropy', 'max_depth': 8}\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:  5.1min finished\n",
      " 86%|████████▌ | 6/7 [05:50<00:58, 58.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6664817143862415\n",
      "Best parameters: {'max_features': 'auto', 'min_samples_split': 30, 'criterion': 'entropy', 'max_depth': 16}\n",
      "Fitting 10 folds for each of 135 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1350 out of 1350 | elapsed: 61.2min finished\n",
      "100%|██████████| 7/7 [1:07:02<00:00, 574.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for validations set: 0.6507336969013348\n",
      "Best parameters: {'learning_rate': 0.005, 'min_samples_leaf': 20, 'subsample': 0.2, 'max_depth': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameter_tuning_all(p222_1, C_range_num = 100, if_scaler = hp.if_scaler, if_remove_icd = hp.if_remove_icd, if_save = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
